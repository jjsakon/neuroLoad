{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local version of LFP setup (shouldn't use for longer sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd; pd.set_option('display.max_columns', 30)\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['pdf.fonttype'] = 42; plt.rcParams['ps.fonttype'] = 42 # fix fonts for Illustrator\n",
    "from copy import copy\n",
    "from scipy import stats,signal\n",
    "from ptsa.data.timeseries import TimeSeries\n",
    "import time\n",
    "import mat73 # this loads .mat files as dicts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unable to parse\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Invalid\")\n",
    "\n",
    "sys.path.append('/Volumes/GoogleDrive/My Drive/Fried/python_code/modules/')\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from general import *\n",
    "\n",
    "import neuralynx_io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the electrode names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'LSTG', 'PZ', 'RA', 'LA', 'HR', 'LOF', 'RMH', 'LFSG', 'Ez', 'LOPR', 'ROF', 'LAC', 'RAI', 'LMH']\n"
     ]
    }
   ],
   "source": [
    "dir_path = '/Volumes/data/NLData/D566/EXP7_Movie_24_Pre_Sleep/2023-08-05_22-00-04/'\n",
    "dir_path = '/Volumes/data/NLData/D566/EXP9_Movie_24_Post_Sleep/2023-08-06_08-44-52/'\n",
    "dir_path = '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData'\n",
    "\n",
    "file_extension = '.ncs'\n",
    "min_size_mb = 0.5  # MB\n",
    "max_size_mb = 50  # MB # should not be more than 29.4MB for macros...since that's size after 2 hours when NLX creates a new file\n",
    "\n",
    "files = find_files_in_range(dir_path, file_extension, min_size_mb, max_size_mb)\n",
    "\n",
    "# Extract possible electrode names\n",
    "matches = [re.search(r'([^/]+?)[_.]', os.path.basename(file)).group(1) for file in files] # if you want numbers with electrode names\n",
    "# remove electrode numbers if they exist\n",
    "for i_match,match in enumerate(matches):\n",
    "    if match[-1].isdigit():\n",
    "        matches[i_match] = match[:-1]\n",
    "possible_electrode_names = list(set(matches))\n",
    "print(possible_electrode_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually input the array of the electrode names using the last printout to get all filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA1.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA2.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA3.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA4.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA5.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA6.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA7.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA8.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC1.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC2.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC3.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC4.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC5.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC6.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC7.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC8.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC9.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LFSG1.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LFSG2.ncs',\n",
       " '/Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LFSG3.ncs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Data length: 17255424\n",
      "(which is 143.7952 minutes of 2000 Hz sampled data\n"
     ]
    }
   ],
   "source": [
    "electrode_prefixes = ['LAC', 'LOF', 'LOPR', 'LMH', 'RA', 'LA', 'LFSG', 'RMH', 'ROF', 'LSTG', 'RAI']\n",
    "\n",
    "filtered_files = [filename for filename in files if any(os.path.basename(filename).startswith(prefix) for prefix in electrode_prefixes)]\n",
    "filtered_files[0:20]\n",
    "\n",
    "# create a mask that identifies the last electrode in each group so can skip that one for bipolar\n",
    "\n",
    "def extract_info(filepath):\n",
    "    basename = os.path.basename(filepath)\n",
    "    match = re.search(r\"([A-Za-z]+)(\\d+)_?\\d*\\.ncs$\", basename)\n",
    "    if match:\n",
    "        prefix, number = match.groups()\n",
    "        return prefix, int(number), filepath\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected filename format: {basename}\")\n",
    "\n",
    "file_info = [extract_info(path) for path in filtered_files]\n",
    "\n",
    "# Identify the max number for each prefix\n",
    "max_numbers = {}\n",
    "for prefix, number, _ in file_info:\n",
    "    if prefix not in max_numbers:\n",
    "        max_numbers[prefix] = number\n",
    "    else:\n",
    "        max_numbers[prefix] = max(max_numbers[prefix], number)\n",
    "\n",
    "# Generate the mask based on these maximum channel numbers\n",
    "last_electrode_mask = [1 if number == max_numbers[prefix] else 0 for prefix, number, _ in file_info]\n",
    "print(last_electrode_mask)\n",
    "\n",
    "print('Data length: '+str(len(neuralynx_io.load_ncs(filtered_files[0])['data'])))\n",
    "print('(which is '+str(len(neuralynx_io.load_ncs(filtered_files[0])['data'])/2000/60)+' minutes of 2000 Hz sampled data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load macro LFP, decimate it to 1 kHz, and create a TimeSeries object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LA8.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LAC9.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LFSG7.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LMH7.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LOF8.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LOPR7.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/LSTG7.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/RA8.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/RAI6.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/RMH7.ncs since we're doing bipolar referencing\n",
      "skipping /Volumes/data/NLData/D566/EXP4_Morpheme_blocks 4,6,1-3/Blocks 4,6,1-3/2023-08-04_13-27-55/combinedData/ROF8.ncs since we're doing bipolar referencing\n",
      "Shape of LFP matrix (channel X time):\n",
      "(71, 8627712)\n",
      "Time to run: 398.1660568714142\n"
     ]
    }
   ],
   "source": [
    "# from ptsa.data.timeseries import TimeSeries\n",
    "\n",
    "bipolar_ref = 1\n",
    "desired_samplerate = 1000 # Hz. 2000 Hz is default. Decimating to 1000 Hz takes ~15% longer\n",
    "# time_range = range(int(1e6),int(2e6)) # range(0,len(neuralynx_io.load_ncs(filtered_files[0])['data']))\n",
    "time_range = range(0,len(neuralynx_io.load_ncs(filtered_files[0])['data'])) # full data\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "current_sr = neuralynx_io.load_ncs(filtered_files[0])['sampling_rate'] # NLX micros at 32K, macros at 2k\n",
    "lfp_mat = []\n",
    "for i_file in range(len(filtered_files)):\n",
    "    if bipolar_ref == 1:\n",
    "        if last_electrode_mask[i_file] == 0:\n",
    "            temp_mat = neuralynx_io.load_ncs(filtered_files[i_file])['data'][time_range] - \\\n",
    "                neuralynx_io.load_ncs(filtered_files[i_file+1])['data'][time_range] \n",
    "        else:\n",
    "            print(\"skipping \"+filtered_files[i_file]+\" since we're doing bipolar referencing\")\n",
    "            continue # go on to next electrode since we have all the bipolar pairs for this one\n",
    "    else:\n",
    "        temp_mat = neuralynx_io.load_ncs(filtered_files[i_file])['data'][time_range]\n",
    "        \n",
    "    temp_mat = signal.decimate(temp_mat,int(current_sr/desired_samplerate)) \n",
    "    lfp_mat = superVstack(lfp_mat,temp_mat)\n",
    "      \n",
    "time_in_sec = np.linspace(1, np.shape(lfp_mat)[1], np.shape(lfp_mat)[1]) / desired_samplerate\n",
    "channels = np.arange(np.shape(lfp_mat)[0])\n",
    "\n",
    "lfp_mat = TimeSeries(lfp_mat,\n",
    "                dims=('channel', 'time'),\n",
    "                coords={'channel':channels,\n",
    "                        'time':time_in_sec,\n",
    "                        'samplerate':desired_samplerate})\n",
    "print('Shape of LFP matrix (channel X time):')\n",
    "print(np.shape(lfp_mat))\n",
    "\n",
    "print('Time to run: '+str(time.time()-begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter for line noise, grab power at given frequencies, and append across frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptsa.data.filters import ButterworthFilter\n",
    "from ptsa.data.filters import MorletWaveletFilter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# buf = 1000  #to remove edge effects during wavelet convolution # not doing this by events so no buffers needed\n",
    "spec_freqs = np.logspace(np.log10(3), np.log10(180), 8) # spectral frequencies used in Ezzyat et al 2018\n",
    "filt_range = [58., 62.] # don't worry about 120 and 180 Hz harmonics since not sampled in spec_freqs\n",
    "\n",
    "lfp_mat = ButterworthFilter(freq_range=filt_range, filt_type='stop', order=4).filter(timeseries=lfp_mat)\n",
    "\n",
    "#     # downsample EEG...could do this if necessary to save processing power since we're going to \n",
    "#     # bin down to 33.3 ms bins eventually anyway\n",
    "#     eeg_ptsa = eeg_ptsa.resampled(resampled_rate=500.)\n",
    "         \n",
    "#Get spectral power\n",
    "\n",
    "pow_wavelet = MorletWaveletFilter(freqs=spec_freqs, width=5, output='power', cpus=2).filter(timeseries=lfp_mat)\n",
    "#output is freqs, events, elecs, and time\n",
    "# e.g. (8, 240, 141, 700) is 8 logarithmically spaced freqs from 3:180, 240 is trials, 141 elecs, 700 samples (1400 ms each typically)\n",
    "\n",
    "#remove buffer period added to beginning and end of each trial\n",
    "#     pows = pow_wavelet.data[:, :, :, int((buf/1000.)*sr):-1*int((buf/1000.)*sr)]  \n",
    "#     pows = pow_wavelet.isel(time = ((pow_wavelet.time>=start_time) & (pow_wavelet.time<=end_time)))\n",
    "#     pows = np.log10(pows)\n",
    "\n",
    "pow_wavelet = np.log10(pow_wavelet)\n",
    "\n",
    "#     #Average power across the whole interval\n",
    "avg_pows = pow_wavelet.mean('time')\n",
    "#     avg_pows = np.squeeze(avg_pows)\n",
    "\n",
    "z_pows = (pow_wavelet-avg_pows)/pow_wavelet.std('time')\n",
    "# z_pows = zscore(pow_wavelet, axis='time')  #z-score across time\n",
    "#     z_pows = zscore(avg_pows, axis=1)  #z-score power values across events (important to do within-session)\n",
    "#     # freqs X trials X electrodes (e.g. 8 X 240 X 141)\n",
    "#     z_pows = (avg_pows-avg_pows.mean('event'))/avg_pows.std('event')\n",
    "\n",
    "print(np.shape(z_pows)) # output is frequency X channel X time\n",
    "# append frequencies together to create time X feature matrix\n",
    "z_pows = np.array(z_pows).swapaxes(0,2).swapaxes(1,2).reshape(np.shape(pow_wavelet)[2], -1)\n",
    "print(np.shape(z_pows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_num = np.shape(pow_wavelet)[1]\n",
    "plt.figure(figsize=(15, 4.5)); ax=plt.subplot(111)\n",
    "ax1 = ax.matshow(z_pows.swapaxes(0,1), aspect='auto', cmap='RdBu_r', vmin=-3, vmax=3)\n",
    "plt.ylabel(str(elec_num)+' electrodes appended 8x by frequencies')\n",
    "plt.xlabel('Time (ms)')\n",
    "for ln in range(8):    \n",
    "    plt.plot([0, len(z_pows)],[elec_num*ln, elec_num*ln],color=(0.5,0.5,0.5)) \n",
    "plt.colorbar(ax1)\n",
    "print(\"Seconds since started compiling = \", time.time()-begin_time)\n",
    "a=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroLoad_env",
   "language": "python",
   "name": "neuroload_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
